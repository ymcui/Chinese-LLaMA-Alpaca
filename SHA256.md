# SHA256

为了保证文件的完整性，请一定要检查下列文件SHA256值的一致性。

To ensure the completeness of the model, please check the folllowing SHA256 before using them.

### Original LLaMA (consolidated.*.pth)

下表展示了Facebook发布的原版英文LLaMA的SHA256。

The followings are SHA256 values for the original LLaMA files.

| Model (.pth) | Parts | SHA256                                                       |
| ------------ | :---: | ------------------------------------------------------------ |
| 7B           |  00   | 700df0d3013b703a806d2ae7f1bfb8e59814e3d06ae78be0c66368a50059f33d |
| 13B          |  00   | 745bf4e29a4dd6f411e72976d92b452da1b49168a4f41c951cfcc8051823cf08 |
| 13B          |  01   | d5ccbcc465c71c0de439a5aeffebe8344c68a519bce70bc7f9f92654ee567085 |

### Our LLaMA/Alpaca Model

#### Tokenizer.model

下表展示了`tokenizer.model`的SHA256。请注意LLaMA与Alpaca的`tokenizer.model`不同。对于同一个模型类型，不同大小或者版本的`tokenizer.model`是相同的。例如，LLaMA-7B, LLaMA-13B, LLaMA-Plus-7B的`tokenizer.model`相同。

The followings are SHA256 values for `tokenizer.model` files. Note that `tokenizer.model` for LLaMA and Alpaca differ. However, different sizes or versions of LLaMA/Alpaca have the same `tokenizer.model`. For example, LLaMA-7B, LLaMA-13B, LLaMA-Plus-7B's `tokenizer.model` are the same.

| Model Type                | SHA256                                                       |
| ------------------------- | ------------------------------------------------------------ |
| LLaMA (7B, 13B, Plus-7B)  | e2676d4ca29ca1750f6ff203328d73b189321dc5776ceede037cbd36541d70c0 |
| Alpaca (7B, 13B, Plus-7B) | 2d967e855b1213a439df6c8ce2791f869c84b4f3b6cfacf22b86440b8192a2f8 |

#### LoRA weight file: adapter_model.bin 

下表展示了LoRA主体权重文件`adapter_model.bin`的SHA256。

The followings are SHA256 values for `adapter_model.bin`  files.

| LoRA Model (adapter_model.bin) | SHA256                                                       |
| ------------------------------ | ------------------------------------------------------------ |
| Chinese-LLaMA-7B               | 2a2c24d096f5d509f24946fdbd8c25e1ce4a0acb955902f7436d74c0c0379d86 |
| Chinese-LLaMA-Plus-7B          | 8c928db86b2a0cf73f019832f921eb7e1e069ca21441b4bfa12c4381c6cc46be |
| Chinese-LLaMA-13B              | 6a4ce789d219bde122f8d9a20371937f2aa2ee86a2311d9f5e303df2e774f9fc |
| Chinese-LLaMA-Plus-13B         | 784fcff9c4bdf4e77d442a01158e121caf8fcce0f97ffb32396fe7a3617ee7e8 |
| Chinese-Alpaca-7B              | 0d9b6ed8e4a7d1ae590a16c89a452a488d66ff07e45487972f61c2b6e46e36de |
| Chinese-Alpaca-Plus-7B         | 4ee0bf805c312a9a771624d481fbdb4485e1b0a70cd2a8da9f96137f177b795d |
| Chinese-Alpaca-13B             | cb8dda3c005f3343a0740dcd7237fbb600cb14b6bff9b6f3d488c086a2f08ada |
| Chinese-Alpaca-Plus-13B        |                                                              |


### Merged files (consolidated.*.pth)

~下表展示了合并LoRA权重后的全量模型权重（PyTorch版）的SHA256。~

~The followings are SHA256 values for merged files (`consolidated.00.pth`) in the [last merging step](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/README.md#step-2-合并lora权重生成全量模型权重).~

因Transformers库更新频繁，合并后的模型权重SHA256可能随Transformers更新而发生变化，不具有普遍参考价值，因此我们不再展示合并后模型的SHA256。

Due to the frequent updates of the Transformers library, the SHA256 of the merged model may change with the update of Transformers and is not stable, so we no longer show the SHA256 of the merged model.

### How To Check SHA256

In MacOS,

```
> shasum -a 256 your-model-file
```

In Linux, 

```
> sha256sum your-model-file
```

In Windows,

```
> certutil -hashfile your-model-file sha256
```
