{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 转换并量化中文LLaMA/Alpaca模型\n",
        "\n",
        "🎉🎉🎉 **新：现在免费用户也有机会能够转换7B和13B模型了！**\n",
        "\n",
        "💡 提示和小窍门：\n",
        "- 免费用户默认的内存只有12G左右，**笔者用免费账号实测选择TPU的话有机会随机出35G内存**，建议多试几次。如果能随机出25G内存以上的机器就可以了转换7B模型了，35G内存以上机器就能转换13B模型了\n",
        "- Pro(+)用户请选择 “代码执行程序” -> “更改运行时类型” -> “高RAM”\n",
        "- 实测：转换7B级别模型，25G内存的机器就够了；转换13B级别模型需要30G以上的内存（程序莫名崩掉或断开连接就说明内存爆了）\n",
        "- 如果选了“高RAM”之后内存还是不够大的话，选择以下操作，有的时候会分配出很高内存的机器，祝你好运😄！\n",
        "    - 可以把GPU或者TPU也选上（虽然不会用到）\n",
        "    - 选GPU时，Pro用户可选“高级”类型GPU\n",
        "\n",
        "以下信息配置信息供参考（Pro订阅下测试），运行时规格设置为“高RAM”时的设备配置如下（有随机性）：\n",
        "\n",
        "| 硬件加速器  |  RAM  |  硬盘  |\n",
        "| :-- | :--: | :--: |\n",
        "| None | 25GB | 225GB |\n",
        "| TPU | 35GB | 225GB |\n",
        "| GPU（标准，T4）| 25GB | 166GB |\n",
        "| GPU（高性能，V100）| 25GB | 166GB |\n",
        "| GPU（高性能，A100）| **80GB** | 166GB |\n",
        "\n",
        "*温馨提示：用完之后注意断开运行时，选择满足要求的最低配置即可，避免不必要的计算单元消耗（Pro只给100个计算单元）。*"
      ],
      "metadata": {
        "id": "B1c96_k3MahN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装相关依赖"
      ],
      "metadata": {
        "id": "vScqHD_jMFOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5WKFJXIL6ZU",
        "outputId": "c6cb6373-7a09-4f09-a6b6-0e539fa96002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-o3zgxcjp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-o3zgxcjp\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit edb704b26e79b6be03061ba45f340f0acd1f8ab1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6892087 sha256=8546886aaade6911330d1715e55702b2259a516a9fcda79efb0f5426e736b38d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bv7e5hyf/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.28.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting peft\n",
            "  Downloading peft-0.2.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from peft) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from peft) (2.0.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from peft) (5.9.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from peft) (4.28.0.dev0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from peft) (23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from peft) (6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (3.10.7)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.18.0 peft-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install peft\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 克隆目录和代码"
      ],
      "metadata": {
        "id": "ygb1xFIMNQKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEJh7NJNXz9",
        "outputId": "7958734d-5b1c-4d38-85ea-d1082aa674ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca'...\n",
            "remote: Enumerating objects: 327, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/58)\u001b[K\rremote: Counting objects:   3% (2/58)\u001b[K\rremote: Counting objects:   5% (3/58)\u001b[K\rremote: Counting objects:   6% (4/58)\u001b[K\rremote: Counting objects:   8% (5/58)\u001b[K\rremote: Counting objects:  10% (6/58)\u001b[K\rremote: Counting objects:  12% (7/58)\u001b[K\rremote: Counting objects:  13% (8/58)\u001b[K\rremote: Counting objects:  15% (9/58)\u001b[K\rremote: Counting objects:  17% (10/58)\u001b[K\rremote: Counting objects:  18% (11/58)\u001b[K\rremote: Counting objects:  20% (12/58)\u001b[K\rremote: Counting objects:  22% (13/58)\u001b[K\rremote: Counting objects:  24% (14/58)\u001b[K\rremote: Counting objects:  25% (15/58)\u001b[K\rremote: Counting objects:  27% (16/58)\u001b[K\rremote: Counting objects:  29% (17/58)\u001b[K\rremote: Counting objects:  31% (18/58)\u001b[K\rremote: Counting objects:  32% (19/58)\u001b[K\rremote: Counting objects:  34% (20/58)\u001b[K\rremote: Counting objects:  36% (21/58)\u001b[K\rremote: Counting objects:  37% (22/58)\u001b[K\rremote: Counting objects:  39% (23/58)\u001b[K\rremote: Counting objects:  41% (24/58)\u001b[K\rremote: Counting objects:  43% (25/58)\u001b[K\rremote: Counting objects:  44% (26/58)\u001b[K\rremote: Counting objects:  46% (27/58)\u001b[K\rremote: Counting objects:  48% (28/58)\u001b[K\rremote: Counting objects:  50% (29/58)\u001b[K\rremote: Counting objects:  51% (30/58)\u001b[K\rremote: Counting objects:  53% (31/58)\u001b[K\rremote: Counting objects:  55% (32/58)\u001b[K\rremote: Counting objects:  56% (33/58)\u001b[K\rremote: Counting objects:  58% (34/58)\u001b[K\rremote: Counting objects:  60% (35/58)\u001b[K\rremote: Counting objects:  62% (36/58)\u001b[K\rremote: Counting objects:  63% (37/58)\u001b[K\rremote: Counting objects:  65% (38/58)\u001b[K\rremote: Counting objects:  67% (39/58)\u001b[K\rremote: Counting objects:  68% (40/58)\u001b[K\rremote: Counting objects:  70% (41/58)\u001b[K\rremote: Counting objects:  72% (42/58)\u001b[K\rremote: Counting objects:  74% (43/58)\u001b[K\rremote: Counting objects:  75% (44/58)\u001b[K\rremote: Counting objects:  77% (45/58)\u001b[K\rremote: Counting objects:  79% (46/58)\u001b[K\rremote: Counting objects:  81% (47/58)\u001b[K\rremote: Counting objects:  82% (48/58)\u001b[K\rremote: Counting objects:  84% (49/58)\u001b[K\rremote: Counting objects:  86% (50/58)\u001b[K\rremote: Counting objects:  87% (51/58)\u001b[K\rremote: Counting objects:  89% (52/58)\u001b[K\rremote: Counting objects:  91% (53/58)\u001b[K\rremote: Counting objects:  93% (54/58)\u001b[K\rremote: Counting objects:  94% (55/58)\u001b[K\rremote: Counting objects:  96% (56/58)\u001b[K\rremote: Counting objects:  98% (57/58)\u001b[K\rremote: Counting objects: 100% (58/58)\u001b[K\rremote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 327 (delta 7), reused 5 (delta 4), pack-reused 269\u001b[K\n",
            "Receiving objects: 100% (327/327), 10.52 MiB | 22.62 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1347, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 1347 (delta 0), reused 1 (delta 0), pack-reused 1344\u001b[K\n",
            "Receiving objects: 100% (1347/1347), 1.12 MiB | 9.55 MiB/s, done.\n",
            "Resolving deltas: 100% (839/839), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并模型（以Alpaca-7B为例）\n",
        "\n",
        "**⚠️ 再次提醒：7B模型需要25G内存，13B模型需要35G+内存。**\n",
        "\n",
        "此处使用的是🤗模型库中提供的基模型（已是HF格式），而不是Facebook官方的LLaMA模型，因此略去将原版LLaMA转换为HF格式的步骤。\n",
        "\n",
        "**这里直接运行第二步：合并LoRA权重**，生成全量模型权重。可以直接指定🤗模型库的地址，也可以是本地存放地址。\n",
        "- 基模型：`decapoda-research/llama-7b-hf` *（use at your own risk）*\n",
        "- LoRA模型：`ziqingyang/chinese-alpaca-lora-7b`\n",
        "\n",
        "💡 转换13B模型提示：\n",
        "- 请将参数`--base_model`和`--lora_model`中的`7b`改为`13b`即可\n",
        "- 请将`model_size`改为`13B`\n",
        "- **免费用户必须增加一个参数`--offload_dir`以缓解内存压力**，例如`--offload_dir ./offload_temp`\n",
        "\n",
        "该过程比较耗时（下载+转换），需要几分钟到十几分钟不等，请耐心等待。\n",
        "转换好的模型存放在`alpaca-combined`目录。\n",
        "如果你不需要量化模型，那么到这一步就结束了。"
      ],
      "metadata": {
        "id": "nIyxX0DSNsgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora.py \\\n",
        "    --base_model 'decapoda-research/llama-7b-hf' \\\n",
        "    --lora_model 'ziqingyang/chinese-alpaca-lora-7b' \\\n",
        "    --model_size 7B \\\n",
        "    --output_dir alpaca-combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AV4EW5hNhVV",
        "outputId": "cc86c184-4d79-42e8-9e57-312151052507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 01:25:14.472606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading tokenizer.model: 100% 758k/758k [00:00<00:00, 12.8MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 14.4kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 166/166 [00:00<00:00, 62.4kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 427/427 [00:00<00:00, 56.7kB/s]\n",
            "Downloading (…)model.bin.index.json: 100% 25.5k/25.5k [00:00<00:00, 953kB/s]\n",
            "Downloading shards:   0% 0/33 [00:00<?, ?it/s]\n",
            "Downloading (…)l-00001-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 84.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 132MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 161MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 176MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 186MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  28% 115M/405M [00:00<00:01, 192MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  34% 136M/405M [00:00<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  39% 157M/405M [00:00<00:01, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  44% 178M/405M [00:00<00:01, 201MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  49% 199M/405M [00:01<00:01, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  54% 220M/405M [00:01<00:00, 203MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  60% 241M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  65% 262M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  70% 283M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  75% 304M/405M [00:01<00:00, 203MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  80% 325M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  85% 346M/405M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  91% 367M/405M [00:01<00:00, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin: 100% 405M/405M [00:02<00:00, 190MB/s]\n",
            "Downloading shards:   3% 1/33 [00:02<01:14,  2.32s/it]\n",
            "Downloading (…)l-00002-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 86.5MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 147MB/s] \u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 173MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 186MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  28% 115M/405M [00:00<00:01, 188MB/s] \u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  34% 136M/405M [00:00<00:01, 194MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  39% 157M/405M [00:00<00:01, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  44% 178M/405M [00:00<00:01, 195MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  49% 199M/405M [00:01<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  54% 220M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  60% 241M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  65% 262M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  70% 283M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  75% 304M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  80% 325M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  85% 346M/405M [00:01<00:00, 201MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  91% 367M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin: 100% 405M/405M [00:02<00:00, 193MB/s]\n",
            "Downloading shards:   6% 2/33 [00:04<01:11,  2.31s/it]\n",
            "Downloading (…)l-00003-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 95.6MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 146MB/s] \u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 163MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 178MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 184MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  28% 115M/405M [00:00<00:01, 188MB/s] \u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  34% 136M/405M [00:00<00:01, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  39% 157M/405M [00:00<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  44% 178M/405M [00:00<00:01, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  49% 199M/405M [00:01<00:01, 194MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  54% 220M/405M [00:01<00:00, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  60% 241M/405M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  65% 262M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  70% 283M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  75% 304M/405M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  80% 325M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  88% 357M/405M [00:01<00:00, 201MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  93% 377M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin: 100% 405M/405M [00:02<00:00, 190MB/s]\n",
            "Downloading shards:   9% 3/33 [00:06<01:09,  2.31s/it]\n",
            "Downloading (…)l-00004-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 88.4MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 143MB/s] \u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 165MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 177MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 184MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  28% 115M/405M [00:00<00:01, 188MB/s] \u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  34% 136M/405M [00:00<00:01, 190MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  39% 157M/405M [00:00<00:01, 175MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  44% 178M/405M [00:01<00:01, 165MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  49% 199M/405M [00:01<00:01, 173MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  54% 220M/405M [00:01<00:01, 172MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  60% 241M/405M [00:01<00:00, 176MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  65% 262M/405M [00:01<00:00, 180MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  70% 283M/405M [00:01<00:00, 185MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  75% 304M/405M [00:01<00:00, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  80% 325M/405M [00:01<00:00, 189MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  85% 346M/405M [00:01<00:00, 190MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  91% 367M/405M [00:02<00:00, 190MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin: 100% 405M/405M [00:02<00:00, 179MB/s]\n",
            "Downloading shards:  12% 4/33 [00:09<01:08,  2.37s/it]\n",
            "Downloading (…)l-00005-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 91.8MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 151MB/s] \u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 174MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 183MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  26% 105M/405M [00:00<00:01, 195MB/s] \u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  31% 126M/405M [00:00<00:01, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  36% 147M/405M [00:00<00:01, 201MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  41% 168M/405M [00:00<00:01, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  47% 189M/405M [00:00<00:01, 204MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  52% 210M/405M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  57% 231M/405M [00:01<00:00, 195MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  62% 252M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  67% 273M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  73% 294M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  78% 315M/405M [00:01<00:00, 204MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  83% 336M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  88% 357M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  93% 377M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin: 100% 405M/405M [00:02<00:00, 195MB/s]\n",
            "Downloading shards:  15% 5/33 [00:11<01:05,  2.34s/it]\n",
            "Downloading (…)l-00006-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 96.0MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 146MB/s] \u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 173MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 184MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 191MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  28% 115M/405M [00:00<00:01, 195MB/s] \u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  34% 136M/405M [00:00<00:01, 176MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  39% 157M/405M [00:00<00:01, 179MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  44% 178M/405M [00:00<00:01, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  49% 199M/405M [00:01<00:01, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  54% 220M/405M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  60% 241M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  65% 262M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  70% 283M/405M [00:01<00:00, 204MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  75% 304M/405M [00:01<00:00, 206MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  80% 325M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  85% 346M/405M [00:01<00:00, 202MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  91% 367M/405M [00:01<00:00, 203MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin: 100% 405M/405M [00:02<00:00, 189MB/s]\n",
            "Downloading shards:  18% 6/33 [00:13<01:02,  2.33s/it]\n",
            "Downloading (…)l-00007-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 90.3MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 146MB/s] \u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 167MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 178MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 182MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  28% 115M/405M [00:00<00:01, 185MB/s] \u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  34% 136M/405M [00:00<00:01, 181MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  39% 157M/405M [00:00<00:01, 185MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  44% 178M/405M [00:01<00:01, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  49% 199M/405M [00:01<00:01, 190MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  54% 220M/405M [00:01<00:00, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  60% 241M/405M [00:01<00:00, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  65% 262M/405M [00:01<00:00, 188MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  70% 283M/405M [00:01<00:00, 190MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  75% 304M/405M [00:01<00:00, 191MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  80% 325M/405M [00:01<00:00, 191MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  85% 346M/405M [00:01<00:00, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  91% 367M/405M [00:02<00:00, 183MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin: 100% 405M/405M [00:02<00:00, 182MB/s]\n",
            "Downloading shards:  21% 7/33 [00:16<01:01,  2.36s/it]\n",
            "Downloading (…)l-00008-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 93.6MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 152MB/s] \u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 173MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 183MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 188MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  28% 115M/405M [00:00<00:01, 191MB/s] \u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  34% 136M/405M [00:00<00:01, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  39% 157M/405M [00:00<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  44% 178M/405M [00:00<00:01, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  49% 199M/405M [00:01<00:01, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  54% 220M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  60% 241M/405M [00:01<00:00, 188MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  65% 262M/405M [00:01<00:00, 189MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  70% 283M/405M [00:01<00:00, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  75% 304M/405M [00:01<00:00, 191MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  80% 325M/405M [00:01<00:00, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  85% 346M/405M [00:01<00:00, 195MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  91% 367M/405M [00:01<00:00, 191MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin: 100% 405M/405M [00:02<00:00, 188MB/s]\n",
            "Downloading shards:  24% 8/33 [00:18<00:58,  2.35s/it]\n",
            "Downloading (…)l-00009-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 60.4MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 91.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 97.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 97.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  26% 105M/405M [00:01<00:03, 98.7MB/s] \u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  28% 115M/405M [00:01<00:02, 99.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  31% 126M/405M [00:01<00:02, 99.8MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  34% 136M/405M [00:01<00:02, 99.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  36% 147M/405M [00:01<00:02, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  39% 157M/405M [00:01<00:02, 98.0MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  41% 168M/405M [00:01<00:02, 99.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  44% 178M/405M [00:01<00:02, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  47% 189M/405M [00:01<00:02, 98.7MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  49% 199M/405M [00:02<00:02, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  52% 210M/405M [00:02<00:02, 96.8MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  54% 220M/405M [00:02<00:01, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  57% 231M/405M [00:02<00:01, 97.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  60% 241M/405M [00:02<00:01, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  62% 252M/405M [00:02<00:01, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  65% 262M/405M [00:02<00:01, 99.1MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  67% 273M/405M [00:02<00:01, 99.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  70% 283M/405M [00:02<00:01, 99.9MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  73% 294M/405M [00:03<00:01, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  75% 304M/405M [00:03<00:01, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  78% 315M/405M [00:03<00:00, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  80% 325M/405M [00:03<00:00, 97.1MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  83% 336M/405M [00:03<00:00, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  85% 346M/405M [00:03<00:00, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  88% 357M/405M [00:03<00:00, 96.4MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  91% 367M/405M [00:03<00:00, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  93% 377M/405M [00:03<00:00, 97.1MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  96% 388M/405M [00:04<00:00, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin: 100% 405M/405M [00:04<00:00, 96.3MB/s]\n",
            "Downloading shards:  27% 9/33 [00:23<01:11,  2.99s/it]\n",
            "Downloading (…)l-00010-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:   3% 10.5M/405M [00:00<00:07, 55.2MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 71.2MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 78.7MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  10% 41.9M/405M [00:00<00:04, 85.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 90.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 95.6MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  26% 105M/405M [00:01<00:03, 96.0MB/s] \u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  28% 115M/405M [00:01<00:03, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  31% 126M/405M [00:01<00:02, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  34% 136M/405M [00:01<00:03, 86.2MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  36% 147M/405M [00:01<00:02, 90.1MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  39% 157M/405M [00:01<00:02, 91.8MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  41% 168M/405M [00:01<00:02, 93.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  44% 178M/405M [00:01<00:02, 91.1MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  47% 189M/405M [00:02<00:02, 92.1MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  49% 199M/405M [00:02<00:02, 91.6MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  52% 210M/405M [00:02<00:02, 92.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  54% 220M/405M [00:02<00:01, 93.7MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  57% 231M/405M [00:02<00:01, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  60% 241M/405M [00:02<00:01, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  62% 252M/405M [00:02<00:01, 94.2MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  65% 262M/405M [00:02<00:01, 86.7MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  67% 273M/405M [00:03<00:01, 87.2MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  70% 283M/405M [00:03<00:01, 90.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  73% 294M/405M [00:03<00:01, 93.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  75% 304M/405M [00:03<00:01, 88.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  78% 315M/405M [00:03<00:01, 88.6MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  80% 325M/405M [00:03<00:00, 90.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  83% 336M/405M [00:03<00:00, 89.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  88% 357M/405M [00:03<00:00, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  91% 367M/405M [00:04<00:00, 94.2MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  93% 377M/405M [00:04<00:00, 84.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  96% 388M/405M [00:04<00:00, 87.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin: 100% 405M/405M [00:04<00:00, 90.1MB/s]\n",
            "Downloading shards:  30% 10/33 [00:27<01:20,  3.52s/it]\n",
            "Downloading (…)l-00011-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:   3% 10.5M/405M [00:00<00:08, 47.3MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:   5% 21.0M/405M [00:00<00:07, 49.1MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:   8% 31.5M/405M [00:00<00:07, 51.8MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  10% 41.9M/405M [00:00<00:06, 59.5MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  13% 52.4M/405M [00:00<00:05, 60.9MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 57.1MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  18% 73.4M/405M [00:01<00:05, 58.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  21% 83.9M/405M [00:01<00:05, 60.1MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  23% 94.4M/405M [00:01<00:04, 66.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  26% 105M/405M [00:01<00:04, 74.4MB/s] \u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  28% 115M/405M [00:01<00:03, 72.5MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  31% 126M/405M [00:01<00:03, 72.5MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  34% 136M/405M [00:02<00:03, 71.4MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  36% 147M/405M [00:02<00:03, 70.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  39% 157M/405M [00:02<00:03, 66.9MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  41% 168M/405M [00:02<00:03, 62.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  44% 178M/405M [00:02<00:03, 62.8MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  47% 189M/405M [00:02<00:03, 67.5MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  49% 199M/405M [00:03<00:03, 67.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  52% 210M/405M [00:04<00:08, 23.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  54% 220M/405M [00:04<00:06, 29.3MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  60% 241M/405M [00:04<00:04, 40.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  62% 252M/405M [00:04<00:03, 41.5MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  65% 262M/405M [00:05<00:02, 47.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  67% 273M/405M [00:05<00:02, 54.4MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  70% 283M/405M [00:05<00:02, 55.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  73% 294M/405M [00:05<00:01, 58.8MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  75% 304M/405M [00:05<00:01, 64.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  78% 315M/405M [00:05<00:01, 65.0MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  80% 325M/405M [00:05<00:01, 68.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  83% 336M/405M [00:06<00:01, 66.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  85% 346M/405M [00:06<00:00, 67.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  88% 357M/405M [00:06<00:00, 67.6MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  91% 367M/405M [00:06<00:00, 67.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  93% 377M/405M [00:06<00:00, 58.8MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  96% 388M/405M [00:06<00:00, 62.0MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  98% 398M/405M [00:07<00:00, 61.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin: 100% 405M/405M [00:07<00:00, 56.0MB/s]\n",
            "Downloading shards:  33% 11/33 [00:35<01:43,  4.72s/it]\n",
            "Downloading (…)l-00012-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 57.0MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 75.5MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 93.0MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 94.1MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 90.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 91.1MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  26% 105M/405M [00:01<00:03, 91.7MB/s] \u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  28% 115M/405M [00:01<00:03, 91.2MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  31% 126M/405M [00:01<00:02, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  34% 136M/405M [00:01<00:02, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  36% 147M/405M [00:01<00:02, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  39% 157M/405M [00:01<00:02, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  41% 168M/405M [00:01<00:02, 90.1MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  47% 189M/405M [00:02<00:02, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  49% 199M/405M [00:02<00:02, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  52% 210M/405M [00:02<00:02, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  54% 220M/405M [00:02<00:01, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  57% 231M/405M [00:02<00:01, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  60% 241M/405M [00:02<00:01, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  62% 252M/405M [00:02<00:01, 94.4MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  65% 262M/405M [00:02<00:01, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  67% 273M/405M [00:02<00:01, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  70% 283M/405M [00:03<00:01, 95.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  73% 294M/405M [00:03<00:01, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  75% 304M/405M [00:03<00:01, 96.4MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  78% 315M/405M [00:03<00:00, 95.9MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  80% 325M/405M [00:03<00:00, 97.3MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  83% 336M/405M [00:03<00:00, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  85% 346M/405M [00:03<00:00, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  88% 357M/405M [00:03<00:00, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  91% 367M/405M [00:03<00:00, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  93% 377M/405M [00:04<00:00, 95.6MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  96% 388M/405M [00:04<00:00, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin: 100% 405M/405M [00:04<00:00, 93.8MB/s]\n",
            "Downloading shards:  36% 12/33 [00:39<01:38,  4.68s/it]\n",
            "Downloading (…)l-00013-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 57.9MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 76.0MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 99.8MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 101MB/s] \u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  23% 94.4M/405M [00:00<00:03, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  26% 105M/405M [00:01<00:02, 101MB/s] \u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  28% 115M/405M [00:01<00:02, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  31% 126M/405M [00:01<00:02, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  34% 136M/405M [00:01<00:02, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  36% 147M/405M [00:01<00:02, 92.8MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  39% 157M/405M [00:01<00:02, 96.0MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  41% 168M/405M [00:01<00:02, 98.3MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  44% 178M/405M [00:01<00:02, 99.2MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  47% 189M/405M [00:01<00:02, 98.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  49% 199M/405M [00:02<00:02, 99.5MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  52% 210M/405M [00:02<00:02, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  54% 220M/405M [00:02<00:01, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  57% 231M/405M [00:02<00:01, 97.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  60% 241M/405M [00:02<00:01, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  62% 252M/405M [00:02<00:01, 98.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  65% 262M/405M [00:02<00:01, 95.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  67% 273M/405M [00:02<00:01, 97.5MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  70% 283M/405M [00:02<00:01, 98.3MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  73% 294M/405M [00:03<00:01, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  75% 304M/405M [00:03<00:01, 98.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  78% 315M/405M [00:03<00:00, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  80% 325M/405M [00:03<00:00, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  83% 336M/405M [00:03<00:00, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  85% 346M/405M [00:03<00:00, 99.1MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  88% 357M/405M [00:03<00:00, 99.8MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  91% 367M/405M [00:03<00:00, 101MB/s] \u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  93% 377M/405M [00:03<00:00, 97.5MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  96% 388M/405M [00:03<00:00, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin: 100% 405M/405M [00:04<00:00, 96.9MB/s]\n",
            "Downloading shards:  39% 13/33 [00:44<01:31,  4.58s/it]\n",
            "Downloading (…)l-00014-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 58.9MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 73.2MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  10% 41.9M/405M [00:00<00:04, 90.3MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 92.6MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 94.2MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 94.4MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 89.2MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 91.4MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  26% 105M/405M [00:01<00:03, 93.9MB/s] \u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  28% 115M/405M [00:01<00:03, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  31% 126M/405M [00:01<00:02, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  34% 136M/405M [00:01<00:02, 95.4MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  36% 147M/405M [00:01<00:02, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  39% 157M/405M [00:01<00:02, 97.7MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  41% 168M/405M [00:01<00:02, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  44% 178M/405M [00:01<00:02, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  47% 189M/405M [00:02<00:02, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  49% 199M/405M [00:02<00:02, 94.8MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  52% 210M/405M [00:02<00:02, 96.0MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  54% 220M/405M [00:02<00:01, 93.5MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  57% 231M/405M [00:02<00:01, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  60% 241M/405M [00:02<00:01, 96.8MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  62% 252M/405M [00:02<00:01, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  65% 262M/405M [00:02<00:01, 99.1MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  67% 273M/405M [00:02<00:01, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  70% 283M/405M [00:03<00:01, 97.5MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  73% 294M/405M [00:03<00:01, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  75% 304M/405M [00:03<00:01, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  78% 315M/405M [00:03<00:00, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  80% 325M/405M [00:03<00:00, 89.8MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  83% 336M/405M [00:03<00:00, 89.5MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  85% 346M/405M [00:03<00:00, 90.4MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  88% 357M/405M [00:03<00:00, 91.5MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  91% 367M/405M [00:03<00:00, 84.4MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  93% 377M/405M [00:04<00:00, 87.8MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  96% 388M/405M [00:04<00:00, 90.3MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin: 100% 405M/405M [00:04<00:00, 92.6MB/s]\n",
            "Downloading shards:  42% 14/33 [00:48<01:26,  4.58s/it]\n",
            "Downloading (…)l-00015-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 60.0MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 77.4MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 94.4MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  16% 62.9M/405M [00:00<00:04, 77.0MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  18% 73.4M/405M [00:00<00:04, 80.9MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  21% 83.9M/405M [00:01<00:03, 83.6MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 85.9MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  26% 105M/405M [00:01<00:03, 89.3MB/s] \u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  28% 115M/405M [00:01<00:03, 92.2MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  31% 126M/405M [00:01<00:02, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  34% 136M/405M [00:01<00:02, 89.5MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  39% 157M/405M [00:01<00:02, 94.5MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  41% 168M/405M [00:01<00:03, 78.6MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  44% 178M/405M [00:02<00:02, 83.3MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  47% 189M/405M [00:02<00:02, 87.8MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  49% 199M/405M [00:02<00:02, 90.9MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  52% 210M/405M [00:02<00:02, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  54% 220M/405M [00:02<00:01, 94.4MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  57% 231M/405M [00:02<00:01, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  60% 241M/405M [00:02<00:01, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  62% 252M/405M [00:02<00:01, 79.9MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  65% 262M/405M [00:03<00:01, 85.2MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  67% 273M/405M [00:03<00:01, 83.4MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  70% 283M/405M [00:03<00:01, 87.6MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  73% 294M/405M [00:03<00:01, 90.1MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  75% 304M/405M [00:03<00:01, 74.6MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  80% 325M/405M [00:03<00:00, 87.5MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  85% 346M/405M [00:03<00:00, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  88% 357M/405M [00:04<00:00, 82.2MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  91% 367M/405M [00:04<00:00, 83.9MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  93% 377M/405M [00:04<00:00, 87.0MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  96% 388M/405M [00:04<00:00, 89.8MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin: 100% 405M/405M [00:04<00:00, 87.2MB/s]\n",
            "Downloading shards:  45% 15/33 [00:53<01:23,  4.66s/it]\n",
            "Downloading (…)l-00016-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 67.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 70.1MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  10% 41.9M/405M [00:00<00:04, 87.3MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  13% 52.4M/405M [00:00<00:04, 86.0MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 90.3MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 93.0MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 91.8MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  26% 105M/405M [00:01<00:03, 92.4MB/s] \u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  28% 115M/405M [00:01<00:03, 93.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  31% 126M/405M [00:01<00:02, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  34% 136M/405M [00:01<00:02, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  36% 147M/405M [00:01<00:02, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  39% 157M/405M [00:01<00:02, 91.2MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  41% 168M/405M [00:01<00:02, 92.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  44% 178M/405M [00:01<00:02, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  47% 189M/405M [00:02<00:02, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  49% 199M/405M [00:02<00:02, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  52% 210M/405M [00:02<00:02, 94.5MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  54% 220M/405M [00:02<00:01, 96.1MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  57% 231M/405M [00:02<00:01, 94.4MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  60% 241M/405M [00:02<00:01, 91.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  62% 252M/405M [00:02<00:01, 93.5MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  65% 262M/405M [00:02<00:01, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  67% 273M/405M [00:02<00:01, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  70% 283M/405M [00:03<00:01, 96.7MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  73% 294M/405M [00:03<00:01, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  75% 304M/405M [00:03<00:01, 93.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  78% 315M/405M [00:03<00:00, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  80% 325M/405M [00:03<00:00, 93.3MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  83% 336M/405M [00:03<00:00, 92.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  85% 346M/405M [00:03<00:00, 92.2MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  88% 357M/405M [00:03<00:00, 91.7MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  91% 367M/405M [00:03<00:00, 93.2MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  93% 377M/405M [00:04<00:00, 94.8MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  96% 388M/405M [00:04<00:00, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin: 100% 405M/405M [00:04<00:00, 92.7MB/s]\n",
            "Downloading shards:  48% 16/33 [00:58<01:18,  4.63s/it]\n",
            "Downloading (…)l-00017-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 75.4MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 73.6MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 93.5MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 98.7MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  26% 105M/405M [00:01<00:03, 98.8MB/s] \u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  28% 115M/405M [00:01<00:02, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  31% 126M/405M [00:01<00:02, 93.0MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  34% 136M/405M [00:01<00:03, 89.1MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  36% 147M/405M [00:01<00:02, 92.8MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  39% 157M/405M [00:01<00:02, 95.4MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  41% 168M/405M [00:01<00:02, 94.5MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  47% 189M/405M [00:01<00:02, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  49% 199M/405M [00:02<00:02, 97.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  52% 210M/405M [00:02<00:01, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  54% 220M/405M [00:02<00:01, 98.8MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  57% 231M/405M [00:02<00:01, 97.2MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  60% 241M/405M [00:02<00:01, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  62% 252M/405M [00:02<00:01, 94.8MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  65% 262M/405M [00:02<00:01, 93.6MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  67% 273M/405M [00:02<00:01, 91.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  70% 283M/405M [00:02<00:01, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  73% 294M/405M [00:03<00:01, 91.4MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  75% 304M/405M [00:03<00:01, 90.6MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  78% 315M/405M [00:03<00:00, 90.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  80% 325M/405M [00:03<00:00, 90.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  83% 336M/405M [00:03<00:00, 91.7MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  85% 346M/405M [00:03<00:00, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  88% 357M/405M [00:03<00:00, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  91% 367M/405M [00:03<00:00, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  93% 377M/405M [00:04<00:00, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  96% 388M/405M [00:04<00:00, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin: 100% 405M/405M [00:04<00:00, 93.8MB/s]\n",
            "Downloading shards:  52% 17/33 [01:02<01:13,  4.60s/it]\n",
            "Downloading (…)l-00018-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 70.3MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 79.6MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 85.1MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  10% 41.9M/405M [00:00<00:04, 89.2MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 93.3MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 92.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 89.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 91.2MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  26% 105M/405M [00:01<00:03, 92.8MB/s] \u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  28% 115M/405M [00:01<00:03, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  31% 126M/405M [00:01<00:02, 95.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  34% 136M/405M [00:01<00:02, 95.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  36% 147M/405M [00:01<00:02, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  39% 157M/405M [00:01<00:02, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  41% 168M/405M [00:01<00:02, 97.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  44% 178M/405M [00:01<00:02, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  47% 189M/405M [00:02<00:02, 96.0MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  49% 199M/405M [00:02<00:02, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  52% 210M/405M [00:02<00:02, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  54% 220M/405M [00:02<00:01, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  57% 231M/405M [00:02<00:01, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  60% 241M/405M [00:02<00:01, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  62% 252M/405M [00:02<00:01, 99.2MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  65% 262M/405M [00:02<00:01, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  67% 273M/405M [00:02<00:01, 99.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  70% 283M/405M [00:02<00:01, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  73% 294M/405M [00:03<00:01, 95.8MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  75% 304M/405M [00:03<00:01, 95.6MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  78% 315M/405M [00:03<00:00, 97.2MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  80% 325M/405M [00:03<00:00, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  83% 336M/405M [00:03<00:00, 92.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  85% 346M/405M [00:03<00:00, 94.8MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  88% 357M/405M [00:03<00:00, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  91% 367M/405M [00:03<00:00, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  93% 377M/405M [00:03<00:00, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  96% 388M/405M [00:04<00:00, 93.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin: 100% 405M/405M [00:04<00:00, 94.2MB/s]\n",
            "Downloading shards:  55% 18/33 [01:07<01:08,  4.57s/it]\n",
            "Downloading (…)l-00019-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 57.6MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 88.1MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  10% 41.9M/405M [00:00<00:04, 89.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  13% 52.4M/405M [00:00<00:04, 88.0MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 91.8MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 93.7MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  26% 105M/405M [00:01<00:03, 98.8MB/s] \u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  28% 115M/405M [00:01<00:02, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  31% 126M/405M [00:01<00:02, 95.2MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  34% 136M/405M [00:01<00:02, 95.7MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  36% 147M/405M [00:01<00:02, 92.2MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  39% 157M/405M [00:01<00:02, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  41% 168M/405M [00:01<00:02, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  44% 178M/405M [00:01<00:02, 93.6MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  47% 189M/405M [00:02<00:02, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  49% 199M/405M [00:02<00:02, 98.0MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  52% 210M/405M [00:02<00:01, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  54% 220M/405M [00:02<00:01, 99.3MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  57% 231M/405M [00:02<00:01, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  60% 241M/405M [00:02<00:01, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  62% 252M/405M [00:02<00:01, 97.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  65% 262M/405M [00:02<00:01, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  67% 273M/405M [00:02<00:01, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  70% 283M/405M [00:02<00:01, 98.8MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  73% 294M/405M [00:03<00:01, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  75% 304M/405M [00:03<00:01, 99.2MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  78% 315M/405M [00:03<00:00, 99.2MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  80% 325M/405M [00:03<00:00, 99.8MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  83% 336M/405M [00:03<00:00, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  85% 346M/405M [00:03<00:00, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  88% 357M/405M [00:03<00:00, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  91% 367M/405M [00:03<00:00, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  93% 377M/405M [00:03<00:00, 97.7MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  96% 388M/405M [00:04<00:00, 97.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin: 100% 405M/405M [00:04<00:00, 96.0MB/s]\n",
            "Downloading shards:  58% 19/33 [01:11<01:03,  4.53s/it]\n",
            "Downloading (…)l-00020-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 73.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 84.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 90.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 93.3MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 97.3MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 92.5MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  26% 105M/405M [00:01<00:03, 98.7MB/s] \u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  28% 115M/405M [00:01<00:02, 99.5MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  31% 126M/405M [00:01<00:02, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  34% 136M/405M [00:01<00:02, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  36% 147M/405M [00:01<00:02, 98.1MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  39% 157M/405M [00:01<00:02, 97.9MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  41% 168M/405M [00:01<00:02, 94.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  44% 178M/405M [00:01<00:02, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  47% 189M/405M [00:01<00:02, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  49% 199M/405M [00:02<00:02, 97.7MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  52% 210M/405M [00:02<00:01, 98.7MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  54% 220M/405M [00:02<00:01, 98.8MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  57% 231M/405M [00:02<00:01, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  60% 241M/405M [00:02<00:01, 97.9MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  62% 252M/405M [00:02<00:01, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  65% 262M/405M [00:02<00:01, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  67% 273M/405M [00:02<00:01, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  70% 283M/405M [00:02<00:01, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  73% 294M/405M [00:03<00:01, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  75% 304M/405M [00:03<00:01, 94.8MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  78% 315M/405M [00:03<00:00, 96.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  80% 325M/405M [00:03<00:00, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  83% 336M/405M [00:03<00:00, 96.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  85% 346M/405M [00:03<00:00, 97.1MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  88% 357M/405M [00:03<00:00, 97.1MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  91% 367M/405M [00:03<00:00, 97.1MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  93% 377M/405M [00:03<00:00, 96.4MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  96% 388M/405M [00:04<00:00, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin: 100% 405M/405M [00:04<00:00, 96.0MB/s]\n",
            "Downloading shards:  61% 20/33 [01:16<00:58,  4.49s/it]\n",
            "Downloading (…)l-00021-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 69.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 81.2MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 85.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  10% 41.9M/405M [00:00<00:04, 89.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 90.9MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 89.5MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 89.5MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 92.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 93.9MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  26% 105M/405M [00:01<00:03, 90.5MB/s] \u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  28% 115M/405M [00:01<00:03, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  31% 126M/405M [00:01<00:02, 93.9MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  34% 136M/405M [00:01<00:02, 92.7MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  36% 147M/405M [00:01<00:02, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  39% 157M/405M [00:01<00:02, 95.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  41% 168M/405M [00:01<00:02, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  44% 178M/405M [00:01<00:02, 95.6MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  47% 189M/405M [00:02<00:02, 91.3MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  49% 199M/405M [00:02<00:02, 93.9MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  52% 210M/405M [00:02<00:02, 89.2MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  54% 220M/405M [00:02<00:02, 92.0MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  57% 231M/405M [00:02<00:01, 93.8MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  60% 241M/405M [00:02<00:01, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  62% 252M/405M [00:02<00:01, 94.5MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  65% 262M/405M [00:02<00:01, 96.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  67% 273M/405M [00:02<00:01, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  70% 283M/405M [00:03<00:01, 98.0MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  73% 294M/405M [00:03<00:01, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  75% 304M/405M [00:03<00:01, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  78% 315M/405M [00:03<00:00, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  80% 325M/405M [00:03<00:00, 92.6MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  83% 336M/405M [00:03<00:00, 87.0MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  85% 346M/405M [00:03<00:00, 90.6MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  88% 357M/405M [00:03<00:00, 93.7MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  91% 367M/405M [00:03<00:00, 89.4MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  93% 377M/405M [00:04<00:00, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  96% 388M/405M [00:04<00:00, 95.7MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin: 100% 405M/405M [00:04<00:00, 92.8MB/s]\n",
            "Downloading shards:  64% 21/33 [01:20<00:54,  4.51s/it]\n",
            "Downloading (…)l-00022-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 61.6MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 78.4MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 95.2MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 98.3MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 97.5MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  23% 94.4M/405M [00:00<00:03, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  26% 105M/405M [00:01<00:03, 99.2MB/s] \u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  28% 115M/405M [00:01<00:02, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  31% 126M/405M [00:01<00:02, 97.2MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  34% 136M/405M [00:01<00:02, 97.2MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  36% 147M/405M [00:01<00:02, 93.6MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  39% 157M/405M [00:01<00:02, 93.6MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  41% 168M/405M [00:01<00:02, 92.4MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  44% 178M/405M [00:01<00:02, 92.4MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  47% 189M/405M [00:02<00:02, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  49% 199M/405M [00:02<00:02, 95.6MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  52% 210M/405M [00:02<00:02, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  54% 220M/405M [00:02<00:01, 96.7MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  57% 231M/405M [00:02<00:01, 97.5MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  60% 241M/405M [00:02<00:01, 98.0MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  62% 252M/405M [00:02<00:01, 99.3MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  65% 262M/405M [00:02<00:01, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  67% 273M/405M [00:02<00:01, 97.3MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  70% 283M/405M [00:02<00:01, 98.8MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  73% 294M/405M [00:03<00:01, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  75% 304M/405M [00:03<00:01, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  78% 315M/405M [00:03<00:00, 92.6MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  80% 325M/405M [00:03<00:00, 95.0MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  83% 336M/405M [00:03<00:00, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  85% 346M/405M [00:03<00:00, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  88% 357M/405M [00:03<00:00, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  91% 367M/405M [00:03<00:00, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  93% 377M/405M [00:03<00:00, 95.8MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  96% 388M/405M [00:04<00:00, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin: 100% 405M/405M [00:04<00:00, 95.3MB/s]\n",
            "Downloading shards:  67% 22/33 [01:25<00:49,  4.49s/it]\n",
            "Downloading (…)l-00023-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 67.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 81.7MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 90.7MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 91.3MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 94.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 96.3MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 96.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 88.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 91.6MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  26% 105M/405M [00:01<00:03, 93.2MB/s] \u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  28% 115M/405M [00:01<00:03, 93.3MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  31% 126M/405M [00:01<00:02, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  34% 136M/405M [00:01<00:03, 82.1MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  36% 147M/405M [00:01<00:03, 85.9MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  39% 157M/405M [00:01<00:02, 89.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  41% 168M/405M [00:01<00:02, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  44% 178M/405M [00:01<00:02, 95.9MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  47% 189M/405M [00:02<00:02, 96.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  49% 199M/405M [00:02<00:02, 91.2MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  52% 210M/405M [00:02<00:02, 93.0MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  54% 220M/405M [00:02<00:01, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  57% 231M/405M [00:02<00:01, 92.4MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  60% 241M/405M [00:02<00:01, 94.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  62% 252M/405M [00:02<00:01, 91.9MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  65% 262M/405M [00:02<00:01, 94.6MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  67% 273M/405M [00:02<00:01, 94.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  70% 283M/405M [00:03<00:01, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  73% 294M/405M [00:03<00:01, 93.7MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  75% 304M/405M [00:03<00:01, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  78% 315M/405M [00:03<00:00, 95.7MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  80% 325M/405M [00:03<00:00, 97.3MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  83% 336M/405M [00:03<00:00, 97.7MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  85% 346M/405M [00:03<00:00, 95.4MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  88% 357M/405M [00:03<00:00, 92.3MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  91% 367M/405M [00:03<00:00, 92.4MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  93% 377M/405M [00:04<00:00, 95.4MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  96% 388M/405M [00:04<00:00, 95.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin: 100% 405M/405M [00:04<00:00, 93.0MB/s]\n",
            "Downloading shards:  70% 23/33 [01:29<00:45,  4.51s/it]\n",
            "Downloading (…)l-00024-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 72.8MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 87.1MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 97.1MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 97.6MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  23% 94.4M/405M [00:00<00:03, 99.6MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  26% 105M/405M [00:01<00:02, 101MB/s]  \u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  28% 115M/405M [00:01<00:02, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  31% 126M/405M [00:01<00:02, 98.0MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  34% 136M/405M [00:01<00:02, 91.7MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  36% 147M/405M [00:01<00:02, 94.3MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  39% 157M/405M [00:01<00:02, 94.7MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  41% 168M/405M [00:01<00:02, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  44% 178M/405M [00:01<00:02, 97.4MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  47% 189M/405M [00:01<00:02, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  49% 199M/405M [00:02<00:02, 98.3MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  52% 210M/405M [00:02<00:01, 99.7MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  54% 220M/405M [00:02<00:01, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  57% 231M/405M [00:02<00:01, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  60% 241M/405M [00:02<00:01, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  62% 252M/405M [00:02<00:01, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  65% 262M/405M [00:02<00:01, 99.6MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  67% 273M/405M [00:02<00:01, 99.3MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  70% 283M/405M [00:02<00:01, 98.6MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  73% 294M/405M [00:03<00:01, 99.1MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  75% 304M/405M [00:03<00:01, 96.1MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  78% 315M/405M [00:03<00:00, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  80% 325M/405M [00:03<00:00, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  83% 336M/405M [00:03<00:00, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  85% 346M/405M [00:03<00:00, 96.7MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  88% 357M/405M [00:03<00:00, 98.1MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  91% 367M/405M [00:03<00:00, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  93% 377M/405M [00:03<00:00, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  96% 388M/405M [00:03<00:00, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin: 100% 405M/405M [00:04<00:00, 97.4MB/s]\n",
            "Downloading shards:  73% 24/33 [01:33<00:40,  4.46s/it]\n",
            "Downloading (…)l-00025-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:   3% 10.5M/405M [00:00<00:05, 70.5MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:   5% 21.0M/405M [00:00<00:04, 85.9MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:   8% 31.5M/405M [00:00<00:03, 93.5MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  10% 41.9M/405M [00:00<00:03, 95.1MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 96.5MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  18% 73.4M/405M [00:00<00:03, 97.2MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  21% 83.9M/405M [00:00<00:03, 96.6MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  23% 94.4M/405M [00:00<00:03, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  26% 105M/405M [00:01<00:03, 99.1MB/s] \u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  28% 115M/405M [00:01<00:02, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  31% 126M/405M [00:01<00:02, 96.2MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  34% 136M/405M [00:01<00:03, 86.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  36% 147M/405M [00:01<00:02, 89.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  39% 157M/405M [00:01<00:02, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  41% 168M/405M [00:01<00:02, 96.0MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  44% 178M/405M [00:01<00:02, 96.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  47% 189M/405M [00:01<00:02, 96.9MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  49% 199M/405M [00:02<00:02, 97.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  52% 210M/405M [00:02<00:01, 99.0MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  54% 220M/405M [00:02<00:01, 99.3MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  57% 231M/405M [00:02<00:01, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  62% 252M/405M [00:02<00:01, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  67% 273M/405M [00:02<00:01, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  70% 283M/405M [00:02<00:01, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  73% 294M/405M [00:03<00:01, 98.5MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  75% 304M/405M [00:03<00:01, 98.2MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  78% 315M/405M [00:03<00:00, 99.9MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  80% 325M/405M [00:03<00:00, 99.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  83% 336M/405M [00:03<00:00, 101MB/s] \u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  85% 346M/405M [00:03<00:00, 97.7MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  88% 357M/405M [00:03<00:00, 98.4MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  91% 367M/405M [00:03<00:00, 99.7MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  93% 377M/405M [00:03<00:00, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  96% 388M/405M [00:03<00:00, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin: 100% 405M/405M [00:04<00:00, 97.5MB/s]\n",
            "Downloading shards:  76% 25/33 [01:38<00:35,  4.43s/it]\n",
            "Downloading (…)l-00026-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:   3% 10.5M/405M [00:00<00:09, 42.5MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:   5% 21.0M/405M [00:00<00:06, 56.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:   8% 31.5M/405M [00:00<00:05, 62.3MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  10% 41.9M/405M [00:00<00:07, 45.6MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 52.8MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  16% 62.9M/405M [00:01<00:06, 49.4MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  18% 73.4M/405M [00:01<00:07, 46.7MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  21% 83.9M/405M [00:01<00:06, 52.8MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  23% 94.4M/405M [00:01<00:06, 50.5MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  26% 105M/405M [00:02<00:05, 53.1MB/s] \u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  28% 115M/405M [00:02<00:05, 51.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  31% 126M/405M [00:02<00:05, 49.6MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  34% 136M/405M [00:02<00:05, 47.7MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  36% 147M/405M [00:02<00:04, 51.8MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  39% 157M/405M [00:03<00:04, 57.5MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  41% 168M/405M [00:03<00:04, 58.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  44% 178M/405M [00:03<00:04, 56.4MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  47% 189M/405M [00:03<00:03, 62.1MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  49% 199M/405M [00:03<00:03, 66.1MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  52% 210M/405M [00:03<00:03, 63.7MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  54% 220M/405M [00:03<00:02, 67.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  57% 231M/405M [00:04<00:02, 64.3MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  60% 241M/405M [00:04<00:02, 60.6MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  62% 252M/405M [00:04<00:02, 57.7MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  65% 262M/405M [00:04<00:02, 62.2MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  67% 273M/405M [00:04<00:02, 52.4MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  70% 283M/405M [00:05<00:02, 56.9MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  73% 294M/405M [00:05<00:01, 56.8MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  75% 304M/405M [00:05<00:01, 57.7MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  78% 315M/405M [00:05<00:01, 49.1MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  80% 325M/405M [00:05<00:01, 52.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  83% 336M/405M [00:06<00:01, 50.1MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  85% 346M/405M [00:06<00:01, 48.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  88% 357M/405M [00:06<00:01, 46.5MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  91% 367M/405M [00:06<00:00, 45.8MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  93% 377M/405M [00:07<00:00, 51.1MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  96% 388M/405M [00:07<00:00, 53.5MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin: 100% 405M/405M [00:07<00:00, 53.8MB/s]\n",
            "Downloading shards:  79% 26/33 [01:46<00:38,  5.43s/it]\n",
            "Downloading (…)l-00027-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:   3% 10.5M/405M [00:00<00:08, 45.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:   5% 21.0M/405M [00:00<00:06, 55.2MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:   8% 31.5M/405M [00:00<00:08, 42.1MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  10% 41.9M/405M [00:00<00:08, 42.0MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 44.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  16% 62.9M/405M [00:01<00:07, 47.2MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  18% 73.4M/405M [00:01<00:07, 42.0MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  21% 83.9M/405M [00:01<00:07, 44.4MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  23% 94.4M/405M [00:02<00:06, 45.1MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  26% 105M/405M [00:02<00:06, 46.3MB/s] \u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  28% 115M/405M [00:02<00:05, 49.9MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  31% 126M/405M [00:02<00:06, 44.7MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  34% 136M/405M [00:02<00:05, 46.1MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  36% 147M/405M [00:03<00:04, 51.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  39% 157M/405M [00:03<00:04, 51.9MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  41% 168M/405M [00:03<00:04, 53.7MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  44% 178M/405M [00:03<00:04, 55.7MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  47% 189M/405M [00:03<00:04, 52.1MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  49% 199M/405M [00:04<00:03, 53.4MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  52% 210M/405M [00:04<00:03, 51.3MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  54% 220M/405M [00:04<00:03, 55.0MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  57% 231M/405M [00:04<00:03, 47.3MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  60% 241M/405M [00:04<00:03, 49.4MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  62% 252M/405M [00:05<00:03, 47.4MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  65% 262M/405M [00:05<00:02, 47.8MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  67% 273M/405M [00:05<00:02, 51.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  70% 283M/405M [00:05<00:02, 48.4MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  73% 294M/405M [00:06<00:02, 49.1MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  75% 304M/405M [00:06<00:02, 47.5MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  78% 315M/405M [00:06<00:01, 49.5MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  80% 325M/405M [00:06<00:01, 52.5MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  83% 336M/405M [00:06<00:01, 50.4MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  85% 346M/405M [00:07<00:01, 50.9MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  88% 357M/405M [00:07<00:01, 46.1MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  91% 367M/405M [00:07<00:00, 48.5MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  93% 377M/405M [00:07<00:00, 53.7MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  96% 388M/405M [00:07<00:00, 51.2MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  98% 398M/405M [00:08<00:00, 53.9MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin: 100% 405M/405M [00:08<00:00, 49.3MB/s]\n",
            "Downloading shards:  82% 27/33 [01:54<00:37,  6.33s/it]\n",
            "Downloading (…)l-00028-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:   3% 10.5M/405M [00:00<00:07, 50.7MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:   5% 21.0M/405M [00:00<00:07, 53.7MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:   8% 31.5M/405M [00:00<00:06, 58.0MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  10% 41.9M/405M [00:00<00:05, 61.9MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  13% 52.4M/405M [00:00<00:05, 62.5MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 61.0MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  18% 73.4M/405M [00:01<00:05, 64.3MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  21% 83.9M/405M [00:01<00:05, 60.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  23% 94.4M/405M [00:01<00:04, 64.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  26% 105M/405M [00:01<00:04, 66.7MB/s] \u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  28% 115M/405M [00:01<00:04, 64.9MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  31% 126M/405M [00:01<00:04, 67.0MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  34% 136M/405M [00:02<00:04, 66.2MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  36% 147M/405M [00:02<00:03, 65.3MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  39% 157M/405M [00:02<00:03, 62.3MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  41% 168M/405M [00:02<00:03, 64.7MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  44% 178M/405M [00:02<00:03, 66.5MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  47% 189M/405M [00:03<00:03, 55.5MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  49% 199M/405M [00:03<00:03, 57.0MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  52% 210M/405M [00:03<00:03, 61.9MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  54% 220M/405M [00:03<00:03, 57.2MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  57% 231M/405M [00:03<00:03, 57.3MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  60% 241M/405M [00:03<00:02, 54.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  62% 252M/405M [00:04<00:02, 55.2MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  65% 262M/405M [00:04<00:02, 56.7MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  67% 273M/405M [00:04<00:02, 59.4MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  70% 283M/405M [00:04<00:02, 51.4MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  73% 294M/405M [00:05<00:02, 46.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  75% 304M/405M [00:05<00:01, 51.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  78% 315M/405M [00:05<00:01, 57.0MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  80% 325M/405M [00:05<00:01, 56.9MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  83% 336M/405M [00:05<00:01, 58.4MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  85% 346M/405M [00:05<00:00, 59.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  88% 357M/405M [00:06<00:00, 62.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  91% 367M/405M [00:06<00:00, 61.0MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  93% 377M/405M [00:06<00:00, 59.8MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  96% 388M/405M [00:06<00:00, 54.8MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin: 100% 405M/405M [00:06<00:00, 59.1MB/s]\n",
            "Downloading shards:  85% 28/33 [02:01<00:32,  6.54s/it]\n",
            "Downloading (…)l-00029-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:   3% 10.5M/405M [00:00<00:06, 64.7MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:   5% 21.0M/405M [00:00<00:05, 68.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:   8% 31.5M/405M [00:00<00:06, 57.9MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  10% 41.9M/405M [00:00<00:06, 58.5MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  13% 52.4M/405M [00:00<00:05, 64.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 62.4MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  18% 73.4M/405M [00:01<00:05, 66.2MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  21% 83.9M/405M [00:01<00:05, 61.9MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  23% 94.4M/405M [00:01<00:05, 53.7MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  26% 105M/405M [00:01<00:06, 47.6MB/s] \u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  28% 115M/405M [00:02<00:05, 51.9MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  31% 126M/405M [00:02<00:05, 55.3MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  34% 136M/405M [00:02<00:05, 53.3MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  36% 147M/405M [00:02<00:04, 56.4MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  39% 157M/405M [00:02<00:04, 58.7MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  41% 168M/405M [00:02<00:03, 62.5MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  44% 178M/405M [00:03<00:04, 55.6MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  47% 189M/405M [00:03<00:03, 60.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  49% 199M/405M [00:03<00:03, 65.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  52% 210M/405M [00:03<00:02, 69.2MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  54% 220M/405M [00:03<00:02, 65.3MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  57% 231M/405M [00:03<00:03, 53.2MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  60% 241M/405M [00:04<00:02, 56.3MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  62% 252M/405M [00:04<00:02, 57.0MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  65% 262M/405M [00:04<00:02, 52.3MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  67% 273M/405M [00:04<00:02, 52.4MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  70% 283M/405M [00:04<00:02, 57.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  73% 294M/405M [00:05<00:01, 57.3MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  75% 304M/405M [00:05<00:01, 62.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  78% 315M/405M [00:05<00:01, 55.1MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  80% 325M/405M [00:05<00:01, 56.6MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  83% 336M/405M [00:05<00:01, 45.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  85% 346M/405M [00:06<00:01, 47.5MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  88% 357M/405M [00:06<00:00, 48.7MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  91% 367M/405M [00:06<00:00, 52.9MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  93% 377M/405M [00:06<00:00, 55.4MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  96% 388M/405M [00:06<00:00, 60.0MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  98% 398M/405M [00:07<00:00, 52.9MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin: 100% 405M/405M [00:07<00:00, 56.3MB/s]\n",
            "Downloading shards:  88% 29/33 [02:08<00:27,  6.79s/it]\n",
            "Downloading (…)l-00030-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 139MB/s] \u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 153MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  18% 73.4M/405M [00:00<00:02, 146MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  23% 94.4M/405M [00:00<00:02, 153MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  28% 115M/405M [00:00<00:01, 160MB/s] \u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  34% 136M/405M [00:00<00:01, 171MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  39% 157M/405M [00:00<00:01, 180MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  44% 178M/405M [00:01<00:01, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  49% 199M/405M [00:01<00:01, 191MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  54% 220M/405M [00:01<00:00, 195MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  60% 241M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  65% 262M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  70% 283M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  75% 304M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  80% 325M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  85% 346M/405M [00:01<00:00, 194MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  91% 367M/405M [00:02<00:00, 189MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin: 100% 405M/405M [00:02<00:00, 181MB/s]\n",
            "Downloading shards:  91% 30/33 [02:11<00:16,  5.50s/it]\n",
            "Downloading (…)l-00031-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 95.8MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 154MB/s] \u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 174MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 179MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 187MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  28% 115M/405M [00:00<00:01, 192MB/s] \u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  34% 136M/405M [00:00<00:01, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  39% 157M/405M [00:00<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  44% 178M/405M [00:00<00:01, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  49% 199M/405M [00:01<00:01, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  54% 220M/405M [00:01<00:00, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  60% 241M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  65% 262M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  70% 283M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  75% 304M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  80% 325M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  85% 346M/405M [00:01<00:00, 194MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  91% 367M/405M [00:01<00:00, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin: 100% 405M/405M [00:02<00:00, 191MB/s]\n",
            "Downloading shards:  94% 31/33 [02:13<00:09,  4.54s/it]\n",
            "Downloading (…)l-00032-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:   3% 10.5M/405M [00:00<00:04, 92.1MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:   8% 31.5M/405M [00:00<00:02, 143MB/s] \u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  13% 52.4M/405M [00:00<00:02, 168MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  18% 73.4M/405M [00:00<00:01, 182MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  23% 94.4M/405M [00:00<00:01, 190MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  28% 115M/405M [00:00<00:01, 189MB/s] \u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  34% 136M/405M [00:00<00:01, 189MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  39% 157M/405M [00:00<00:01, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  44% 178M/405M [00:00<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  49% 199M/405M [00:01<00:01, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  54% 220M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  60% 241M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  65% 262M/405M [00:01<00:00, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  70% 283M/405M [00:01<00:00, 201MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  75% 304M/405M [00:01<00:00, 203MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  80% 325M/405M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  85% 346M/405M [00:01<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  91% 367M/405M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin: 100% 405M/405M [00:02<00:00, 191MB/s]\n",
            "Downloading shards:  97% 32/33 [02:16<00:03,  3.87s/it]\n",
            "Downloading (…)l-00033-of-00033.bin:   0% 0.00/524M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:   2% 10.5M/524M [00:00<00:05, 95.2MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:   6% 31.5M/524M [00:00<00:03, 154MB/s] \u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  10% 52.4M/524M [00:00<00:03, 156MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  14% 73.4M/524M [00:00<00:02, 174MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  18% 94.4M/524M [00:00<00:02, 183MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  22% 115M/524M [00:00<00:02, 185MB/s] \u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  26% 136M/524M [00:00<00:02, 185MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  30% 157M/524M [00:00<00:01, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  34% 178M/524M [00:00<00:01, 194MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  38% 199M/524M [00:01<00:01, 192MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  42% 220M/524M [00:01<00:01, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  48% 252M/524M [00:01<00:01, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  52% 273M/524M [00:01<00:01, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  58% 304M/524M [00:01<00:01, 200MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  62% 325M/524M [00:01<00:01, 193MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  66% 346M/524M [00:01<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  72% 377M/524M [00:01<00:00, 199MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  76% 398M/524M [00:02<00:00, 197MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  80% 419M/524M [00:02<00:00, 196MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  84% 440M/524M [00:02<00:00, 198MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  90% 472M/524M [00:02<00:00, 203MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  94% 493M/524M [00:02<00:00, 204MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin: 100% 524M/524M [00:02<00:00, 192MB/s]\n",
            "Downloading shards: 100% 33/33 [02:18<00:00,  4.21s/it]\n",
            "Loading checkpoint shards: 100% 33/33 [01:17<00:00,  2.35s/it]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 18.0kB/s]\n",
            "Extended vocabulary size: 49954\n",
            "Downloading (…)/adapter_config.json: 100% 472/472 [00:00<00:00, 128kB/s]\n",
            "Downloading adapter_model.bin: 100% 858M/858M [00:08<00:00, 98.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 量化模型\n",
        "接下来我们使用[llama.cpp](https://github.com/ggerganov/llama.cpp)工具对上一步生成的全量版本权重进行转换，生成4-bit量化模型。\n",
        "\n",
        "### 编译工具\n",
        "\n",
        "首先对llama.cpp工具进行编译。"
      ],
      "metadata": {
        "id": "ueexcKo-Q_EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GbjsT2wRRCR",
        "outputId": "cc31fffb-7d92-46a7-d819-8d78bc61bdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型转换为ggml格式（FP16）\n",
        "\n",
        "这一步，我们将模型转换为ggml格式（FP16）。\n",
        "- 在这之前需要把`alpaca-combined`目录挪个位置，并且保证符合转换脚本的要求。\n",
        "- tokenizer文件需要在模型文件的父节点上（注意这里使用的是中文Alpaca模型附带的文件，而不是合并模型步骤转换出来的）。\n",
        "- 这里我们直接从 https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model 下载中文Alpaca-7B的tokenizer.model文件。\n",
        "\n",
        "💡 转换13B模型提示：\n",
        "- tokenizer可以直接用7B的，13B和7B的相同\n",
        "- llama和alpaca的tokenizer不可混用\n",
        "- 以下看到7B字样的都是文件夹名，与转换过程没有关系了，改不改都行"
      ],
      "metadata": {
        "id": "gw2xpYC0RcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && mkdir zh-models && mv ../alpaca-combined zh-models/7B"
      ],
      "metadata": {
        "id": "5KgnFVStRjio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp/zh-models && wget https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl2E2WBPSnmw",
        "outputId": "b2e78420-c998-48c1-8100-5f4b53980b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 01:34:01--  https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model\n",
            "Resolving huggingface.co (huggingface.co)... 52.22.128.237, 52.2.178.255, 34.203.133.210, ...\n",
            "Connecting to huggingface.co (huggingface.co)|52.22.128.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/0f/01/0f01544c04c27e0a0357540e7be5763000a215cedb3be4a0356b56983f2fd5e3/2d967e855b1213a439df6c8ce2791f869c84b4f3b6cfacf22b86440b8192a2f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1680917642&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBmLzAxLzBmMDE1NDRjMDRjMjdlMGEwMzU3NTQwZTdiZTU3NjMwMDBhMjE1Y2VkYjNiZTRhMDM1NmI1Njk4M2YyZmQ1ZTMvMmQ5NjdlODU1YjEyMTNhNDM5ZGY2YzhjZTI3OTFmODY5Yzg0YjRmM2I2Y2ZhY2YyMmI4NjQ0MGI4MTkyYTJmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODA5MTc2NDJ9fX1dfQ__&Signature=hIqJZTQx4mY7A1tW%7EwGXzPCdcudk%7EI9QhowPn0undSprCYp%7EiLgQjgI68qaP1fJ9z98dYVQ7FE3Rsjz67DlYuLNsigA%7E%7ELEnK%7ErRaOU5kLvWJT5zIsT8vSLftgFqdsYu9qKZz5mlYhV9v3nWIFZbtjx9sSkegF1pV06ad45w5x6Ri%7Esiw2vu7MJnfGGpiXq7KgGsviSAE334fYaMt3Ukh3p-vbnDWxY-ZR8QGJ2jF0pjLLtOKpcZwfN3V7IG1mOezqU8arrp91xTf4zYPUhUlnkp6i4IqoEKXnUT0H9W7c5IVZFiRGEFUcOy5j9Ci3a18c4eZiGSt6OSDyB2VCINlQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-04-05 01:34:02--  https://cdn-lfs.huggingface.co/repos/0f/01/0f01544c04c27e0a0357540e7be5763000a215cedb3be4a0356b56983f2fd5e3/2d967e855b1213a439df6c8ce2791f869c84b4f3b6cfacf22b86440b8192a2f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1680917642&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBmLzAxLzBmMDE1NDRjMDRjMjdlMGEwMzU3NTQwZTdiZTU3NjMwMDBhMjE1Y2VkYjNiZTRhMDM1NmI1Njk4M2YyZmQ1ZTMvMmQ5NjdlODU1YjEyMTNhNDM5ZGY2YzhjZTI3OTFmODY5Yzg0YjRmM2I2Y2ZhY2YyMmI4NjQ0MGI4MTkyYTJmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODA5MTc2NDJ9fX1dfQ__&Signature=hIqJZTQx4mY7A1tW%7EwGXzPCdcudk%7EI9QhowPn0undSprCYp%7EiLgQjgI68qaP1fJ9z98dYVQ7FE3Rsjz67DlYuLNsigA%7E%7ELEnK%7ErRaOU5kLvWJT5zIsT8vSLftgFqdsYu9qKZz5mlYhV9v3nWIFZbtjx9sSkegF1pV06ad45w5x6Ri%7Esiw2vu7MJnfGGpiXq7KgGsviSAE334fYaMt3Ukh3p-vbnDWxY-ZR8QGJ2jF0pjLLtOKpcZwfN3V7IG1mOezqU8arrp91xTf4zYPUhUlnkp6i4IqoEKXnUT0H9W7c5IVZFiRGEFUcOy5j9Ci3a18c4eZiGSt6OSDyB2VCINlQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 54.230.18.21, 54.230.18.124, 54.230.18.111, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|54.230.18.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 757972 (740K) [binary/octet-stream]\n",
            "Saving to: ‘tokenizer.model’\n",
            "\n",
            "tokenizer.model     100%[===================>] 740.21K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-04-05 01:34:02 (15.1 MB/s) - ‘tokenizer.model’ saved [757972/757972]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python convert-pth-to-ggml.py zh-models/7B/ 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHeoTMQS1AQ",
        "outputId": "db23cf5c-3310-413e-b3e7-70132c354ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': -1}\n",
            "Namespace(dir_model='zh-models/7B/', ftype=1, vocab_only=0)\n",
            "n_parts = 1\n",
            "\n",
            "Processing part 1 of 1\n",
            "\n",
            "Processing variable: tok_embeddings.weight with shape: (49954, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.0.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.0.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.1.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.1.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.1.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.2.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.2.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.2.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.3.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.3.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.3.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.4.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.4.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.4.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.5.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.5.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.5.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.6.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.6.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.6.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.7.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.7.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.7.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.8.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.8.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.8.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.9.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.9.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.9.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.10.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.10.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.10.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.11.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.11.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.11.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.12.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.12.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.12.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.13.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.13.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.13.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.14.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.14.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.14.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.15.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.15.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.15.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.16.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.16.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.16.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.17.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.17.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.17.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.18.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.18.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.18.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.19.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.19.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.19.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.20.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.20.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.20.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.21.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.21.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.21.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.22.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.22.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.22.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.23.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.23.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.23.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.24.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.24.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.24.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.25.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.25.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.25.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.26.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.26.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.26.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.27.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.27.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.27.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.28.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.28.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.28.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.29.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.29.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.29.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.30.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.30.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.30.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.31.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.31.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.31.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: output.weight with shape: (49954, 4096) and type: torch.float16\n",
            "Done. Output file: zh-models/7B//ggml-model-f16.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 将FP16模型量化为4-bit\n",
        "\n",
        "**⚠️ 本步骤消耗内存峰值为<font size=\"5\">3-4G</font>左右，运行前务必确认是否有足够空闲内存！**\n",
        "\n",
        "我们进一步将FP16模型转换为4-bit量化模型。"
      ],
      "metadata": {
        "id": "hEZEJAVYCHkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xyais7OUVDI",
        "outputId": "39ae131e-caa5-49dd-8292-ca2b701d5752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_model_quantize_internal: loading model from './zh-models/7B/ggml-model-f16.bin'\n",
            "llama_model_quantize_internal: n_vocab = 49954\n",
            "llama_model_quantize_internal: n_ctx   = 512\n",
            "llama_model_quantize_internal: n_embd  = 4096\n",
            "llama_model_quantize_internal: n_mult  = 256\n",
            "llama_model_quantize_internal: n_head  = 32\n",
            "llama_model_quantize_internal: n_layer = 32\n",
            "llama_model_quantize_internal: f16     = 1\n",
            "                           tok_embeddings.weight - [ 4096, 49954], type =    f16 quantizing .. size =   780.53 MB ->   121.96 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.0.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.028 0.046 0.071 0.103 0.137 0.158 0.137 0.103 0.071 0.046 0.028 0.016 0.021 \n",
            "                    layers.0.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.027 0.045 0.071 0.104 0.138 0.158 0.139 0.104 0.071 0.045 0.027 0.016 0.021 \n",
            "                    layers.0.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.018 0.032 0.051 0.076 0.103 0.128 0.141 0.128 0.103 0.075 0.051 0.032 0.019 0.022 \n",
            "                    layers.0.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.028 0.046 0.072 0.105 0.136 0.151 0.136 0.105 0.072 0.046 0.028 0.016 0.021 \n",
            "                 layers.0.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.0.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                 layers.0.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                  layers.0.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.0.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.1.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.051 0.077 0.104 0.127 0.137 0.127 0.104 0.077 0.051 0.032 0.019 0.022 \n",
            "                    layers.1.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.018 0.032 0.051 0.076 0.104 0.128 0.138 0.128 0.104 0.077 0.051 0.032 0.018 0.022 \n",
            "                    layers.1.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.018 0.031 0.051 0.076 0.104 0.129 0.139 0.129 0.104 0.076 0.051 0.031 0.018 0.021 \n",
            "                    layers.1.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.028 0.046 0.071 0.104 0.137 0.154 0.137 0.104 0.071 0.046 0.028 0.016 0.021 \n",
            "                 layers.1.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.1.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.1.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.1.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.1.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.2.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.2.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.051 0.076 0.104 0.127 0.137 0.127 0.104 0.077 0.051 0.032 0.019 0.022 \n",
            "                    layers.2.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.136 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.2.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                 layers.2.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.2.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.2.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.2.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.2.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.3.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                    layers.3.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.136 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                    layers.3.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.3.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.3.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.3.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.3.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.3.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.3.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.4.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.4.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                    layers.4.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.135 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.4.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                 layers.4.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.4.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.4.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.4.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.4.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.5.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.5.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.5.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.5.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.5.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.5.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.5.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.5.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.5.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.6.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.6.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.6.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                    layers.6.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.6.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.6.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.6.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.6.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.6.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.7.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.7.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.7.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.7.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.7.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.7.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.7.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.7.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.7.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.8.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.8.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                    layers.8.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.8.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.8.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.8.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                 layers.8.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.8.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.8.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.9.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.9.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                    layers.9.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.9.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.9.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.9.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                 layers.9.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.9.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.9.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.10.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.10.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.10.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.10.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.10.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.10.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.10.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.10.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.10.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.11.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.11.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.11.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.11.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.11.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.11.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                layers.11.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.11.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.11.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.12.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.12.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.12.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.12.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.12.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.12.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.12.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.12.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.12.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.13.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.13.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                   layers.13.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.13.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.13.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.13.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                layers.13.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.13.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.13.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.14.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.14.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.14.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.14.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.14.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.14.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.14.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.14.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.14.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.15.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.15.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.15.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.15.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.15.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.15.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.15.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.15.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.15.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.16.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.16.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.16.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.16.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.16.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.16.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.126 0.134 0.126 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                layers.16.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.16.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.16.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.17.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.17.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.17.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.17.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.17.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.17.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                layers.17.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.17.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.17.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.18.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.18.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.18.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.18.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.18.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.18.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.18.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.18.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.18.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.19.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.19.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.19.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.19.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.19.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.19.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.19.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.19.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.19.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.20.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.20.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.20.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.20.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.20.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.20.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.20.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.20.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.20.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.21.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.21.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.21.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.21.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.21.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.21.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.21.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.21.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.21.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.22.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.22.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.22.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.22.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.22.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.22.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.22.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.22.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.22.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.23.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.23.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.23.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.23.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.23.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.23.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.23.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.23.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.23.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.24.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.24.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.24.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.24.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.24.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.24.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.24.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.24.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.24.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.25.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.25.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.25.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.25.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.25.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.25.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.25.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.25.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.25.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.26.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.26.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.26.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.26.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.26.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.26.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.26.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.26.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.26.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.27.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.27.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.27.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.27.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.27.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.27.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.27.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.27.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.27.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.28.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.28.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.28.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.28.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.28.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.28.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.28.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.28.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.28.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.29.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.29.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.29.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.29.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.29.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.29.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                layers.29.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.29.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.29.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.30.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.30.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.30.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.30.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.30.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.30.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.018 0.032 0.051 0.076 0.104 0.128 0.137 0.128 0.104 0.076 0.051 0.032 0.018 0.022 \n",
            "                layers.30.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.30.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.30.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.31.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                   layers.31.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.31.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                   layers.31.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.31.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.31.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.021 0.018 0.031 0.050 0.075 0.104 0.130 0.140 0.130 0.104 0.075 0.050 0.031 0.018 0.021 \n",
            "                layers.31.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.31.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.31.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                                     norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                                   output.weight - [ 4096, 49954], type =    f16 quantizing .. size =   780.53 MB ->   121.96 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "llama_model_quantize_internal: model size  = 26266.08 MB\n",
            "llama_model_quantize_internal: quant size  =  4104.93 MB\n",
            "llama_model_quantize_internal: hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "\n",
            "main: quantize time = 181767.03 ms\n",
            "main:    total time = 181767.03 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### （可选）测试量化模型解码\n",
        "至此已完成了所有转换步骤。\n",
        "我们运行一条命令测试一下是否能够正常加载并进行对话。\n",
        "\n",
        "FP16和Q4量化文件存放在./llama.cpp/zh-models/7B下，可按需下载使用。"
      ],
      "metadata": {
        "id": "DLkuRAo9Vkb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./main -m ./zh-models/7B/ggml-model-q4_0.bin --color -f ./prompts/alpaca.txt -p \"详细介绍一下北京的名胜古迹：\" -n 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-ep1BsVQtG",
        "outputId": "fd1aa435-bb3c-462a-9531-d62421442590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: seed = 1680658813\n",
            "llama_model_load: loading model from './zh-models/7B/ggml-model-q4_0.bin' - please wait ...\n",
            "llama_model_load: n_vocab = 49954\n",
            "llama_model_load: n_ctx   = 512\n",
            "llama_model_load: n_embd  = 4096\n",
            "llama_model_load: n_mult  = 256\n",
            "llama_model_load: n_head  = 32\n",
            "llama_model_load: n_layer = 32\n",
            "llama_model_load: n_rot   = 128\n",
            "llama_model_load: f16     = 2\n",
            "llama_model_load: n_ff    = 11008\n",
            "llama_model_load: n_parts = 1\n",
            "llama_model_load: type    = 1\n",
            "llama_model_load: ggml map size = 4105.59 MB\n",
            "llama_model_load: ggml ctx size =  81.25 KB\n",
            "llama_model_load: mem required  = 5897.67 MB (+ 1026.00 MB per state)\n",
            "llama_model_load: loading tensors from './zh-models/7B/ggml-model-q4_0.bin'\n",
            "llama_model_load: model size =  4104.93 MB / num tensors = 291\n",
            "llama_init_from_file: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 4 / 4 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\n",
            "generate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m 详细介绍一下北京的名胜古迹：\u001b[0m\n",
            " 故宫皇院，长城，颐和园，天安门广场，圆明堂，兵马俑博物馆等。 [end of text]\n",
            "\n",
            "llama_print_timings:        load time =  4050.11 ms\n",
            "llama_print_timings:      sample time =    53.26 ms /    32 runs   (    1.66 ms per run)\n",
            "llama_print_timings: prompt eval time =  3882.40 ms /    11 tokens (  352.95 ms per token)\n",
            "llama_print_timings:        eval time = 16815.59 ms /    31 runs   (  542.44 ms per run)\n",
            "llama_print_timings:       total time = 21990.50 ms\n"
          ]
        }
      ]
    }
  ]
}
